data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval.csv",sep=",")
dim(data)
data[1,]
cor(data[,3], data[,4])
data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval.csv",sep=",")
data[1,]
cor(data[,3], data[,4])
data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval.csv",sep=",")
data[1,]
cor(data[,3], data[,4])
q()
data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval.csv",sep=",")
dim(data)
data[1,]
cor(data[,1], data[,3])
cor(data[,1], data[,2])
q()
library(openNLP)
tagPOS("IntelliJ IDEA IDE description talks about 30 day free trial and not about how the tool is better  or worse compared to other IDEs .")
tagPOS(" For example : IntelliJ IDEA IDE description talks about 30 day free trial and not about how the tool is better  or worse compared to other IDEs ")
tagPOS(" For example : IntelliJ IDEA IDE description discusses 30 day free trial and not about how the tool is better  or worse compared to other IDEs ")
tagPOS(" For example : IntelliJ IDEA IDE discusses 30 day free trial and not about how the tool is better  or worse compared to other IDEs ")
data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval.csv",sep=",")
dim(data)
data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval.csv",sep=",")
dim(data)
data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval.csv",sep=",")
dim(data)
cor(data[,1], data[,2])
data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval.csv",sep=",")
dim(data)
data[1,]
cor(data[,1], data[,2])
data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval.csv",sep=",")
dim(data)
cor(data[,1], data[,2])
data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval.csv",sep=",")
dim(data)
cor(data[,1], data[,2])
data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval.csv-1",sep=",")
data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval-1.csv",sep=",")
dim(data)
cor(data[,1], data[,2])
data = read.csv("/Users/lakshmi/Documents/Thesis/Review Coverage 2012/ISD-data/results-coverage-eval.csv",sep=",")
dim(data)
cor(data[,1], data[,2])
help(cor)
cor(data[,1], data[,2], method="spearman")
cor(data[,1], data[,2], method="pearson")
cor(data[,1], data[,2], method="kendall")
cor(data[1:10,1], data[1:10,2], method="spearman")
cor(data[,1], data[,2], method="spearman")
data[1:10,]
cor(data[1:5,1], data[1:5,2], method="spearman")
dim(data)
cor(data[,1], data[,2], method="spearman")
cor(data[,1], data[,2], method="kendall")
q()
library(openNLP)
tagPOS("ethics involve considerations.")
tagPOS("ethics involve considerations")
q()
library(openNLP)
tagPOS("he ran fast")
q()
library(openNLP)
tagPOS("a word order based graph representation")
tagPOS("We use a word order based graph representation")
tagPOS("We use a word order graph representation")
tagPOS("He quickly ran")
q()
data = read.csv("/Users/lakshmi/Documents/Thesis/wordsim353/combined-353-results-WordNet-0.426637.csv",sep=",")
dim(data)
cor(data[,2], data[,3])
cor(data[,2], data[,3], method = "spearman")
cor(data[,2], data[,3], method = "pearson")
exit
q()
dim(data)
dim(myMatrix)
library(e1071)
library(tm)
dataAll = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza-full-patterns/SVM_all.csv", sep=",")
matrix_train = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza-full-patterns/LSA_train.csv",sep=",")
matrix_test = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza-full-patterns/LSA_test.csv",sep=",")
no_training = dim(matrix_train)[1]
no_testing = dim(matrix_test)[1]
#first 666 are training records and next 287 are testing
i = 2;
docs = as.matrix(matrix_train)[1,1];
while(i < no_training + 1)
{
docs = rbind(docs, as.matrix(matrix_train)[i,1])
i = i+1;
}
while(i < no_training + no_testing + 1)
{
docs = rbind(docs, as.matrix(matrix_test)[i - no_training,1])
i = i+1;
}
corp = Corpus(VectorSource(docs));
tdm = TermDocumentMatrix(corp)
myMatrix = inspect(tdm)
data = t(myMatrix) #transpose of the term-doc matrix
dim(data)
data[1,1:10]
data[1,30:700]
data[1,30:70]
data[1,1:30]
q()
library(openNLP)
tagPOS("However, they are also more open to personal interpretation and application")
q()
library(openNLP)
tagPOS("The river bank is far");
q()
ag = read.csv("/Users/lakshmi/Documents/Thesis/Relevance-2012/agreement-correlation-relevance-indira.csv",sep=",")
dim(ag)
ag[190,]
ag[189,]
ag = ag[1:189,]
dim(ag)
cor(ag[,1],ag[,2], method ="spearman")
exit
q()
library(LiblineaR)
library("RWeka")
library(RWeka)
library(e1071)
library(tm)
packages.install("LiblineaR")
install.packages("LiblineaR")
install.packages("RWeka")
install.packages("tm")
library(LiblineaR)
library(RWeka)
matrix_train = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza+SWoRD/Baselines/training-data-patterns.csv", sep=",")
dim(matrix_train)
matrix_test = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza+SWoRD/Baselines/test-data.csv", sep=",")
no_training = dim(matrix_train)[1]
no_testing = dim(matrix_test)[1]
#first 666 are training records and next 287 are testing
i = 2;
docs = as.matrix(matrix_train)[1,1];
while(i < no_training + 1)
{
docs = rbind(docs, as.matrix(matrix_train)[i,1])
i = i+1;
}
while(i < no_training + no_testing + 1)
{
docs = rbind(docs, as.matrix(matrix_test)[i - no_training,1])
i = i+1;
}
corp = Corpus(VectorSource(docs));
tdm = TermDocumentMatrix(corp)
myMatrix = inspect(tdm)
library(tm)
corp = Corpus(VectorSource(docs));
tdm = TermDocumentMatrix(corp)
myMatrix = inspect(tdm)
data = t(myMatrix) 
i = 1
finalMatrix = 0
listColnames = as.vector
while(i < dim(data)[2])
{
if(sum(data[,i]) > 1)
{
finalMatrix = cbind(finalMatrix, data[,i])
if(length(listColnames)  == 0)
listColnames = colnames(data)[i]
else
listColnames = cbind(listColnames, colnames(data)[i])
}
i = i + 1
}
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tdm <- TermDocumentMatrix(corp, control = list(tokenize = TrigramTokenizer))
tdm
myMatrix = inspect(tdm)
dim(myMatrix)
tdm = TermDocumentMatrix(corp)
myMatrix = inspect(tdm)
dim(myMatrix)
no_training
no_testing
matrix_train = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza+SWoRD/Baselines/training-data-patterns.csv", sep=",")
matrix_test = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza+SWoRD/Baselines/test-data-patterns.csv", sep=",")
dim(matrix_test)
matrix_test
dim(matrix_train)
no_training = dim(matrix_train)[1]
no_testing = dim(matrix_test)[1]
#first 666 are training records and next 287 are testing
i = 2;
docs = as.matrix(matrix_train)[1,1];
while(i < no_training + 1)
{
docs = rbind(docs, as.matrix(matrix_train)[i,1])
i = i+1;
}
while(i < no_training + no_testing + 1)
{
docs = rbind(docs, as.matrix(matrix_test)[i - no_training,1])
i = i+1;
}
corp = Corpus(VectorSource(docs));
tdm = TermDocumentMatrix(corp)
myMatrix = inspect(tdm)
data = t(myMatrix)
data
dim(myMatrix)
dim(docs)
dim(matrix_train)
myMatrix
classData = as.matrix(matrix_train[,2])
classData = rbind(classData, as.matrix(matrix_test[,2]))
train = as.matrix(data)[1:no_training,]
test = as.matrix(data)[(no_training+1):(no_training+no_testing),]
no_testing
classTrain = as.matrix(as.numeric(classData[1:no_training]))
classTest = as.matrix(as.numeric(classData[(no_training+1):(no_training+no_testing)]))
colnames(classTrain) = c("Class")
colnames(classTest) = c("Class")
train = cbind(train, classTrain)
test = cbind(test, classTest)
dim(train)
dim(test)
test
train
dim(train)
m = LiblineaR(data=train[,1:127],labels=classTrain,type=6,cost=1,bias=TRUE,verbose=FALSE)
p = predict(m,test[,1:127])
test
test = as.matrix(data)[(no_training+1):(no_training+no_testing),]
test
train = as.matrix(data)[1:no_training,]
classTrain = as.matrix(as.numeric(classData[1:no_training]))
classTest = as.matrix(as.numeric(classData[(no_training+1):(no_training+no_testing)]))
colnames(classTrain) = c("Class")
colnames(classTest) = c("Class")
train = cbind(train, classTrain)
test = cbind(test, classTest)
test
classTest
dim(classTest)
dim(classTrain)
test = as.matrix(data)[(no_training+1):(no_training+no_testing),]
dim(test)
test
as.matrix(test)
t(as.matrix(test))
train
dim(train)
dim(test)
test = t(as.matrix(test))
dim(test)
test = t(test)
dim(test)
test = as.matrix(data)[(no_training+1):(no_training+no_testing),]
test = t(as.matrix(test))
dim(test)
dim(train)
test = cbind(test, classTest)
dim(test)
m = LiblineaR(data=train[,1:127],labels=classTrain,type=6,cost=1,bias=TRUE,verbose=FALSE)
p = predict(m,test[,1:127])
dim(test)
matrix_train = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza+SWoRD/Baselines/training-data-patterns.csv", sep=",")
matrix_test = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza+SWoRD/Baselines/test-data-patterns.csv", sep=",")
no_training = dim(matrix_train)[1]
no_testing = dim(matrix_test)[1]
#first 666 are training records and next 287 are testing
i = 2;
docs = as.matrix(matrix_train)[1,1];
while(i < no_training + 1)
{
docs = rbind(docs, as.matrix(matrix_train)[i,1])
i = i+1;
}
while(i < no_training + no_testing + 1)
{
docs = rbind(docs, as.matrix(matrix_test)[i - no_training,1])
i = i+1;
}
no_training
no_testing
corp = Corpus(VectorSource(docs));
tdm = TermDocumentMatrix(corp)
myMatrix = inspect(tdm)
data = t(myMatrix) 
classData = as.matrix(matrix_train[,2])
classData = rbind(classData, as.matrix(matrix_test[,2]))
#extracting train and test sets
train = as.matrix(data)[1:no_training,]
test = as.matrix(data)[(no_training+1):(no_training+no_testing),]
classTrain = as.matrix(as.numeric(classData[1:no_training]))
classTest = as.matrix(as.numeric(classData[(no_training+1):(no_training+no_testing)]))
dim(train)
dim(test)
colnames(classTrain) = c("Class")
colnames(classTest) = c("Class")
train = cbind(train, classTrain)
test = cbind(test, classTest)
dim(test)
test
dim(train)
m = LiblineaR(data=train[,1:127],labels=classTrain,type=6,cost=1,bias=TRUE,verbose=FALSE)
p = predict(m,test[,1:127])
table(p$predictions, classTest)
m
matrix_train = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza+SWoRD/Baselines/training-data-patterns.csv", sep=",")
matrix_test = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza+SWoRD/Baselines/test-data-patterns.csv", sep=",")
no_training = dim(matrix_train)[1]
no_testing = dim(matrix_test)[1]
#first 666 are training records and next 287 are testing
i = 2;
docs = as.matrix(matrix_train)[1,1];
while(i < no_training + 1)
{
docs = rbind(docs, as.matrix(matrix_train)[i,1])
i = i+1;
}
while(i < no_training + no_testing + 1)
{
docs = rbind(docs, as.matrix(matrix_test)[i - no_training,1])
i = i+1;
}
corp = Corpus(VectorSource(docs));
tdm = TermDocumentMatrix(corp)
myMatrix = inspect(tdm)
data = t(myMatrix)
classData = as.matrix(matrix_train[,2])
classData = rbind(classData, as.matrix(matrix_test[,2]))
#extracting train and test sets
train = as.matrix(data)[1:no_training,]
test = as.matrix(data)[(no_training+1):(no_training+no_testing),]
classTrain = as.matrix(as.numeric(classData[1:no_training]))
classTest = as.matrix(as.numeric(classData[(no_training+1):(no_training+no_testing)]))
colnames(classTrain) = c("Class")
colnames(classTest) = c("Class")
train = cbind(train, classTrain)
test = cbind(test, classTest)
dim(test)
dim(train)
m = LiblineaR(data=train[,1:127],labels=classTrain,type=6,cost=1,bias=TRUE,verbose=FALSE)
p = predict(m,test[,1:127])
table(p$predictions, classTest)
p$predictions
matrix_train = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza+SWoRD/Baselines/training-data-patterns.csv", sep=",")
matrix_test = read.csv("/Users/lakshmi/Documents/Thesis/semantic-patterns-2012/Expertiza+SWoRD/Baselines/test-data-patterns.csv", sep=",")
no_training = dim(matrix_train)[1]
no_testing = dim(matrix_test)[1]
#first 666 are training records and next 287 are testing
i = 2;
docs = as.matrix(matrix_train)[1,1];
while(i < no_training + 1)
{
docs = rbind(docs, as.matrix(matrix_train)[i,1])
i = i+1;
}
while(i < no_training + no_testing + 1)
{
docs = rbind(docs, as.matrix(matrix_test)[i - no_training,1])
i = i+1;
}
corp = Corpus(VectorSource(docs));
tdm = TermDocumentMatrix(corp)
myMatrix = inspect(tdm)
data = t(myMatrix)
classData = as.matrix(matrix_train[,2])
classData = rbind(classData, as.matrix(matrix_test[,2]))
#extracting train and test sets
train = as.matrix(data)[1:no_training,]
test = as.matrix(data)[(no_training+1):(no_training+no_testing),]
classTrain = as.matrix(as.numeric(classData[1:no_training]))
classTest = as.matrix(as.numeric(classData[(no_training+1):(no_training+no_testing)]))
colnames(classTrain) = c("Class")
colnames(classTest) = c("Class")
train = cbind(train, classTrain)
test = cbind(test, classTest)
test
m = LiblineaR(data=train[,1:127],labels=classTrain,type=6,cost=1,bias=TRUE,verbose=FALSE)
p = predict(m,test[,1:127])
p$predictions
q()
